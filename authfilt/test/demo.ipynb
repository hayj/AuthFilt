{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools from external packages:\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from systemtools.number import *\n",
    "from datatools.jsonutils import *\n",
    "from datatools.jsonutils import *\n",
    "from nlptools.basics import *\n",
    "from nlptools.tokenizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools of AuthFilt:\n",
    "from authfilt.buckets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get your data\n",
    "\n",
    "Here a list of tuples\n",
    "Each tuple represent a document and contains the label and a list sentences.\n",
    "A sentence is composed of tokens (so, for this notebook, you need tokenized docs by sentences and words).\n",
    "This file contains unbalanced classes (some have a lot of docs compared to others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  (\n",
      "    cinieâ€¢wordpress.com,\n",
      "    [ [ Yahoo, News, ..., Obama, . ], [ The, Democrat, ..., Bush, . ], ..., [ Which, corrupt, ..., ask, ? ], [ The, ones, ..., stupid, . ] ]\n",
      "  ),\n",
      "  (\n",
      "    cinieâ€¢wordpress.com,\n",
      "    [ [ Face, it, ..., toast, . ], [ Running, as, ..., fail, . ], ..., [ But, you, ..., are, . ], [ That, clearly, ..., math, . ] ]\n",
      "  ),\n",
      "  ...,\n",
      "  (\n",
      "    trendaz.com,\n",
      "    [ [ t, least, ..., reported, . ], [ \", A, ..., dpa, . ], ..., [ Pakistan, tribal, ..., Afghanistan, . ], [ The, country, ..., success, . ] ]\n",
      "  ),\n",
      "  (\n",
      "    trendaz.com,\n",
      "    [ [ (, dpa, ..., Tuesday, . ], [ \", He, ..., Post, . ], ..., [ Mike, Webster, ..., issues, . ], [ Police, filmed, ..., authorities, . ] ]\n",
      "  )\n",
      "]\n",
      "Number of documents: 15098\n"
     ]
    }
   ],
   "source": [
    "# Read your data:\n",
    "dataset = deserialize(\"./data/unbalanced.pickle\")\n",
    "bp(dataset)\n",
    "print(\"Number of documents: \" + str(len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create required data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  03530.com: \n",
      "  {\n",
      "    7515: [ [ A, new, ..., patterns, . ], [ Using, the, ..., words, . ], ..., [ \", The, ..., start, . ], [ Source, :, ..., British, Columbia ] ],\n",
      "    7516: [ [ Tooth, bonding, ..., discoloured, . ], [ It, usually, ..., two, . ], ..., [ Many, people, ..., smile, . ], [ Get, information, ..., dentists, in ] ],\n",
      "    7517: [ [ August, __int_2__, ..., risk, . ], [ The, study, ..., patients, . ], ..., [ \", Low, ..., noted, . ], [ SOURCE, :, ..., Aug., __int_2__ ] ],\n",
      "    7518: [ [ NC, State, ..., setting, . ], [ Dr., Steven, ..., patients, . ], ..., [ A, lot, ..., medicine, . ], [ For, some, ..., humans, . ] ],\n",
      "    7519: [ [ Vitamin, D, ..., results, . ], [ In, this, ..., and, __int_2__ ], ..., [ Annals, of, ..., articles, . ], [ Source, :, ..., of, Physicians ] ],\n",
      "    ...,\n",
      "    7625: [ [ Oxygen, Biotherapeutics, ..., TBI, ) ], [ Oxycyte, is, ..., carrier, . ], ..., [ Additional, information, ..., release, . ], [ This, caution, ..., of, __int_4__ ] ],\n",
      "    7626: [ [ The, Fat, ..., area, . ], [ Most, of, ..., theory, . ], ..., [ Eve, Bryant, ..., Fads, . ], [ In, addition, ..., loss, . ] ],\n",
      "    7627: [ [ Scientists, at, ..., disease, . ], [ Probiotics, are, ..., yogurt, . ], ..., [ For, more, ..., site, . ], [ UC, Davis, ..., Health, System ] ],\n",
      "    7628: [ [ Skeptics, ', ..., charity, . ], [ The, one, ..., deception, ) ], ..., [ Regardless, of, ..., company, . ], [ Come, to, ..., all, . ] ],\n",
      "    7629: [ [ Medical, Research, ..., researchers, . ], [ In, the, ..., disease, . ], ..., [ The, researchers, ..., rearrangements, . ], [ \", It, ..., said, . ] ]\n",
      "  },\n",
      "  1st-guide.org: \n",
      "  {\n",
      "    7233: [ [ by, William, ..., bank, . ], [ A, bank, ..., easily, . ], ..., [ Reasons, For, ..., account, . ], [ If, you, ..., Budgeting, Tips ] ],\n",
      "    7234: [ [ by, Kim, ..., customers, . ], [ Your, customers, ..., gifts, . ], ..., [ Having, a, ..., Deal, ? ], [ Beef, Up, ..., -, Book ] ],\n",
      "    7235: [ [ by, Ohmar, ..., tasks, ? ], [ Increasing, numbers, ..., for, . ], ..., [ by, Zul, ..., belie, ... ], [ Discover, The, ..., Brain, Power ] ],\n",
      "    7236: [ [ by, Nathan, ..., acne, . ], [ And, just, ..., everyone, . ], ..., [ Selecting, the, ..., acne, . ], [ Even, though, ..., Scars, ? ] ],\n",
      "    7237: [ [ by, Thin, ..., health, . ], [ To, get, ..., gym, . ], ..., [ Weight, Loss, ..., supplements, ? ], [ Will, they, ..., Loss, Diets ] ],\n",
      "    ...,\n",
      "    7344: [ [ by, Amy, ..., storm, . ], [ The, reason, ..., stays, . ], ..., [ Things, You, ..., Diet, ? ], [ Fat, Loss, ..., Name, ? ] ],\n",
      "    7345: [ [ by, Caleb, ..., physiology, ? ], [ That, is, ..., women, . ], ..., [ Contributing, factors, ..., most, ... ], [ Causes, ,, ..., off, gynecomastia ] ],\n",
      "    7346: [ [ by, Bob, ..., price, . ], [ When, you, ..., down, . ], ..., [ Global, Resorts, ..., EDC, . ], [ Why, Go, ..., Jamaica, ? ] ],\n",
      "    7347: [ [ by, Matt, ..., site, . ], [ The, control, ..., more, . ], ..., [ Advantages, of, ..., e, ... ], [ Dedicated, Hosting, ..., Hosting, ? ] ],\n",
      "    7348: [ [ by, Markku, ..., true, . ], [ Working, from, ..., perseverance, . ], ..., [ Air, Purifiers, ..., o, ... ], [ Setting, Up, ..., Management, Techniques ] ]\n",
      "  },\n",
      "  adrienneâ€¢livejournal.com: \n",
      "  {\n",
      "    2013: [ [ do, most, ..., system, ? ], [ over, in, ..., books, . ], ..., [ and, she, ..., litre, . ], [ of, course, ..., dollars, ... ] ],\n",
      "    2014: [ [ Pure, self, ..., still, . ], [ Pairing, :, ..., mine, . ], ..., [ He, expects, ..., earth, . ], [ But, the, ..., relief, . ] ]\n",
      "  },\n",
      "  amandaâ€¢livejournal.com: \n",
      "  {\n",
      "    10871: [ [ I, have, ..., London, . ], [ I, wish, ..., longer, . ], ..., [ Had, many, ..., scary, ) ], [ I, think, ..., Soho, . ] ],\n",
      "    10872: [ [ I, totally, ..., soundtrack, . ], [ But, guess, what, ? ], ..., [ But, here, ..., work, . ], [ To, get, ..., work, . ] ],\n",
      "    10873: [ [ this, time, ..., ones, ! ], [ time, passes, ..., what, ! ], ..., [ i, was, ..., so, ! ], [ i, wanted, ..., anything, now ] ],\n",
      "    10874: [ [ I, do, ..., one, . ], [ Every, night, ..., energy, . ], ..., [ Whatever, ,, ..., lame, . ], [ I, should, ..., Eric, . ] ],\n",
      "    10875: [ [ Please, leave, ..., there, . ], [ So, ,, ..., Olympics, . ], ..., [ He, just, ..., going, . ], [ Anyway, ,, ..., that, . ] ],\n",
      "    ...,\n",
      "    10958: [ [ Please, leave, ..., there, . ], [ There, been, ..., China, is ], ..., [ Those, kicking, ..., against, . ], [ Different, degrees, ..., role, . ] ],\n",
      "    10959: [ [ I, finally, ..., gallery, . ], [ You, can, ..., here, . ], ..., [ I, do, ..., set, . ], [ I, should, ..., music, ! ] ],\n",
      "    10960: [ [ so, yea, ..., here, . ], [ some, other, ..., pisei, . ], ..., [ ðŸ˜„, i, ..., days, . ], [ i, do, ..., halp, ? ] ],\n",
      "    10961: [ [ This, trip, ..., Christmas, . ], [ NOT, feeling, ..., ride, . ], ..., [ (, Butterfly, ..., says, . ], [ The, other, ..., stands, . ] ],\n",
      "    10962: [ [ its, so, ..., too, ? ], [ i, never, ..., doing, . ], ..., [ i, miss, ..., puppy, . ], [ i, also, ..., weekend, for ] ]\n",
      "  },\n",
      "  amandaâ€¢wordpress.com: \n",
      "  {\n",
      "    3499: [ [ There, been, ..., stuff, . ], [ I, signed, ..., do, . ], ..., [ There, no, ..., yay, ! ], [ I, still, ..., done, . ] ],\n",
      "    3500: [ [ This, is, ..., quilt, . ], [ It, one, ..., Kelly, . ], ..., [ Here, some, ..., favorites, . ], [ All, photos, ..., pictures, ! ] ],\n",
      "    3501: [ [ Dear, Digital, ..., vileness, . ], [ Yes, ,, ..., plot, . ], ..., [ We, will, ..., yield, . ], [ We, will, ..., news, . ] ],\n",
      "    3502: [ [ I, over, ..., God, ! ], [ I, ca, ..., soups, . ], ..., [ I, walk, ..., takes, . ], [ This, is, ..., now, . ] ],\n",
      "    3503: [ [ Many, astrologers, ..., new, . ], [ With, the, ..., Pluto, . ], ..., [ If, only, ..., it, . ], [ Tomorrow, -, ..., ', charts ] ],\n",
      "    ...,\n",
      "    3514: [ [ from, The, ..., by, D.L. ], [ Moody, Chapter, ..., outside, . ], ..., [ If, he, ..., ambition, . ], [ If, that, ..., way, . ] ],\n",
      "    3515: [ [ La, Villette, ..., aspects, . ], [ Summer, is, ..., eating, . ], ..., [ On, warm, ..., left, ) ], [ You, ca, ..., idyll, . ] ],\n",
      "    3516: [ [ In, __int_4__, ..., debut, . ], [ Her, first, ..., in, __int_4__ ], ..., [ It, seems, ..., medium, . ], [ From, Merriam, ..., for, donations ] ],\n",
      "    3517: [ [ I, hope, ..., weekend, ! ], [ Mine, was, ..., days, . ], ..., [ Tuesday, :, ..., baking, ) ], [ Wednesday, :, ..., Mushroom, Risotto ] ],\n",
      "    3518: [ [ Just, wanted, ..., everything, . ], [ I, finished, ..., time, . ], ..., [ I, be, ..., survey, . ], [ My, flight, ..., point, . ] ]\n",
      "  },\n",
      "  ...,\n",
      "  weblogsinc.com: \n",
      "  {\n",
      "    5134: [ [ Filed, under, ..., controversy, . ], [ St., Louis, ..., here, . ], ..., [ Iif, it, ..., need, ? ], [ But, the, ..., altogether, . ] ],\n",
      "    5135: [ [ Filed, under, ..., boil, . ], [ But, despite, ..., questions, . ], ..., [ Peter, Cohan, ..., Associates, . ], [ He, also, ..., Cohan, Letter ] ],\n",
      "    5136: [ [ Although, there, ..., chains, . ], [ However, ,, ..., shipments, . ], ..., [ Get, quotes, ..., services, . ], [ Get, the, ..., blog, . ] ],\n",
      "    5137: [ [ Filed, under, ..., handset, . ], [ Today, ,, ..., show, . ], ..., [ In, retrospect, ..., factor, . ], [ The, new, ..., fall, . ] ],\n",
      "    5138: [ [ Filed, under, ..., closely, . ], [ Factory, orders, ..., open, . ], ..., [ It, has, ..., redemptions, . ], [ Until, Citigroup, ..., suffer, . ] ],\n",
      "    ...,\n",
      "    5182: [ [ Filed, under, ..., Stack, . ], [ In, his, ..., group, . ], ..., [ According, to, ..., base, . ], [ \", Despite, ..., portfolio, . ] ],\n",
      "    5183: [ [ A, new, ..., Technorati, . ], [ You, can, ..., Yahoo, ! ], ..., [ The, mashup, ..., Shanahan, . ], [ (, BaeBo, ..., too, . ] ],\n",
      "    5184: [ [ I, plopped, ..., NetNewsWire, . ], [ In, fact, ..., FeedDemon, . ], ..., [ I, think, ..., cousins, . ], [ Still, ,, ..., soon, . ] ],\n",
      "    5185: [ [ Filed, under, ..., country, . ], [ Best, Buy, ..., time, . ], ..., [ Although, this, ..., retailer, . ], [ In, August, ..., for, . ] ],\n",
      "    5186: [ [ Filed, under, ..., flames, . ], [ And, in, ..., bargain, . ], ..., [ He, also, ..., The, Cohan ], [ He, has, ..., mentioned, . ] ]\n",
      "  },\n",
      "  weightless dollsâ€¢livejournal.com: \n",
      "  {\n",
      "    7349: [ [ I, was, ..., it, . ], [ it, got, ..., out, . ], ..., [ anyone, know, ? ], [ anyways, ..., ..., folks, . ] ],\n",
      "    7350: [ [ Well, i, ..., boys, . ], [ And, when, ..., looking, . ], ..., [ I, will, ..., now, ! ], [ stay, strong, ..., day, ! ] ],\n",
      "    7351: [ [ Sooo, i, ..., today, . ], [ i, need, ..., it, . ], [ I, hope, ..., impatient, . ], [ well, going, ..., moment, . ] ],\n",
      "    7352: [ [ ugh, ,, ..., failure, . ], [ here, i, ..., toast, . ], ..., [ wish, me, ..., kids, . ], [ i, will, ..., it, . ] ],\n",
      "    7353: [ [ I, have, ..., plan, . ], [ He, obviously, ..., day, . ], ..., [ I, trying, ..., him, . ], [ Do, you, ..., like, __int_4__ ] ],\n",
      "    ...,\n",
      "    7363: [ [ These, are, ..., interesting, ! ], [ If, you, ..., them, ! ], ..., [ Ironing, clothes, ..., burning, . ], [ Just, a, ..., minutes, . ] ],\n",
      "    7364: [ [ so, yeah, ..., saturday, . ], [ damn, football, ..., afterwards, . ], ..., [ lol, . ], [ \", Perfect, ..., soul, . ] ],\n",
      "    7365: [ [ Hey, dudes, ..., today, ! ], [ Scales, budged, ..., food, . ], ..., [ Tell, yourself, ..., you, . ], [ Create, your, ..., it, ! ] ],\n",
      "    7366: [ [ Young, diners, ..., weigh, . ], [ Under, the, ..., scales, . ], ..., [ If, something, ..., too, . ], [ It, would, ..., too, . ] ],\n",
      "    7367: [ [ I, am, ..., ed, ! ], [ so, i, ..., phone, . ], ..., [ what, a, ..., loser, . ], [ he, can, ..., balls, . ] ]\n",
      "  },\n",
      "  wired.com: \n",
      "  {\n",
      "    13224: [ [ The, Department, ..., audit, . ], [ As, a, ..., statutes, . ], ..., [ It, does, ..., says, . ], [ \", Employees, ..., believe, ? ] ],\n",
      "    13225: [ [ Gadget, Lab, ..., projectors, . ], [ In, short, ..., performers, . ], ..., [ This, could, ..., tens, . ], [ Registration, on, ..., Digital, . ] ],\n",
      "    13226: [ [ Android, versions, ..., Monday, . ], [ The, two, ..., earnest, . ], ..., [ The, Daily, ..., store, . ], [ The, issue, ..., Digital, . ] ],\n",
      "    13227: [ [ Playbook, The, ..., end, . ], [ After, seven, ..., finish, . ], ..., [ failed, to, ..., field, . ], [ And, with, ..., Digital, . ] ],\n",
      "    13228: [ [ The, best, ..., it, . ], [ Want, some, ..., life, ? ], ..., [ They, holistic, ..., Flipboard, . ], [ Or, enthralling, ..., chops, . ] ],\n",
      "    ...,\n",
      "    13403: [ [ was, July, ..., trip, . ], [ My, flight, ..., traveling, . ], ..., [ In, any, ..., holiday, . ], [ Lesson, to, ..., supervise, ! ] ],\n",
      "    13404: [ [ Wired, Science, ..., surface, . ], [ It, the, ..., since, . ], ..., [ Piccard, and, ..., father, . ], [ Since, then, ..., Digital, . ] ],\n",
      "    13405: [ [ Autopia, Planes, ..., track, . ], [ Most, drivers, ..., Rimac, . ], ..., [ \", We, ..., it, . ], [ Registration, on, ..., Digital, . ] ],\n",
      "    13406: [ [ Apple, ,, ..., you, . ], [ At, least, ..., think, . ], ..., [ I, ca, ..., people, ! ], [ Registration, on, ..., Digital, . ] ],\n",
      "    13407: [ [ __int_4__, Roselle, ..., service, . ], [ Thomas, Edison, ..., station, . ], ..., [ kinetoscope, ,, ..., projector, . ], [ (, We, ..., Digital, . ] ]\n",
      "  },\n",
      "  xanga.com: \n",
      "  {\n",
      "    10689: [ [ Payday, or, ..., cash, . ], [ While, some, ..., ashamed, . ], ..., [ To, begin, ..., review, . ], [ Answer, all, ..., etc, . ] ],\n",
      "    10690: [ [ view, the, ..., possibilities, . ], [ Staying, right, ..., one, . ], ..., [ She, has, ..., phones, . ], [ Article, phentermine, ..., on, line ] ],\n",
      "    10691: [ [ A, veggie, ..., burger, . ], [ I, bought, ..., Albertsons, . ], ..., [ The, Best, ..., well, . ], [ And, if, ..., link, . ] ],\n",
      "    10692: [ [ energy, and, ..., enthusiasm, . ], [ I, __int_2__, ..., ago, . ], ..., [ Steve, Roberts, ..., practice, . ], [ Get, Insight, ..., no, rx ] ],\n",
      "    10693: [ [ Upon, filing, ..., lawyer, . ], [ Deposition, is, ..., story, . ], ..., [ Johnson, represents, ..., rollovers, . ], [ Call, __int_1__, ..., Bookmark, Button ] ],\n",
      "    ...,\n",
      "    10866: [ [ problems, among, ..., pills, . ], [ You, ve, ..., bars, . ], ..., [ As, always, ..., Pills, . ], [ Article, Source, ..., prescription, ultram ] ],\n",
      "    10867: [ [ The, Blessed, ..., Spain, . ], [ The, Mercedarians, ..., Mercy, ! ], ..., [ She, bound, ..., fray, . ], [ Not, only, ..., Son, . ] ],\n",
      "    10868: [ [ generic, viagraPatients, ..., lavender, . ], [ Clinical, is, ..., fdc, . ], ..., [ President, slowed, ..., methods, . ], [ Her, was, ..., execution, . ] ],\n",
      "    10869: [ [ You, may, ..., bookstore, . ], [ Well, ,, ..., then, . ], ..., [ I, told, ..., hyperglycemic, . ], [ She, did, ..., diabetes, ) ] ],\n",
      "    10870: [ [ When, you, ..., light, . ], [ These, __int_1__, ..., wardrobe, . ], ..., [ Or, you, ..., it, . ], [ This, is, ..., date, . ] ]\n",
      "  },\n",
      "  yachtchartersmagazine.com: \n",
      "  {\n",
      "    3049: [ [ From, the, ..., Trust, \" ], [ The, Trust, ..., November, __int_4__ ], ..., [ Cisco, Cloud, ..., Reserved, . ], [ All, marks, ..., -, CON ] ],\n",
      "    3050: [ [ From, the, ..., Chase, . ], [ Morally, Bankrupt, . ], ..., [ Announces, Cloud, ..., Reserved, . ], [ All, marks, ..., Media, . ] ],\n",
      "    3051: [ [ From, the, ..., stock, . ], [ A, quarterly, ..., Treasury, ) ], ..., [ Parting, the, ..., Reserved, . ], [ All, marks, ..., Media, . ] ],\n",
      "    3052: [ [ Announces, Fourth, ..., States, . ], [ The, company, ..., February, __int_2__ ], ..., [ Top, Two, ..., Reserved, . ], [ All, marks, ..., Media, . ] ],\n",
      "    3053: [ [ From, the, ..., crazy, . ], [ Crazy, fun, ! ], ..., [ My, Top, ..., Reserved, . ], [ All, marks, ..., -, CON ] ],\n",
      "    ...,\n",
      "    3472: [ [ From, the, ..., advisors, . ], [ Ask, yourself, ..., from, __float_1__ ], ..., [ Announces, Cloud, ..., Reserved, . ], [ All, marks, ..., trademarks, of ] ],\n",
      "    3473: [ [ From, the, ..., stock, . ], [ The, quarterly, ..., quarter, . ], ..., [ Announces, Cloud, ..., Reserved, . ], [ All, marks, ..., Media, . ] ],\n",
      "    3474: [ [ From, the, ..., proceedings, . ], [ \", Our, ..., Tronox, . ], ..., [ Announces, Cloud, ..., Reserved, . ], [ All, marks, ..., Media, . ] ],\n",
      "    3475: [ [ From, the, ..., February, __int_2__ ], [ The, dividend, ..., February, __int_2__ ], ..., [ Fares, for, ..., ,, Inc. ], [ Announces, Cloud, ..., Media, . ] ],\n",
      "    3476: [ [ From, the, ..., union, . ], [ The, proposal, ..., vacation, . ], ..., [ Announces, Cloud, ..., Reserved, . ], [ All, marks, ..., -, CON ] ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create a dict mapping class -> ids -> documents.\n",
    "# If you have a very large dataset, you need to compute this using generators or with distributed computing.\n",
    "id = 0\n",
    "documents = dict()\n",
    "for label, sentences in dataset:\n",
    "    if label not in documents:\n",
    "        documents[label] = dict()\n",
    "    documents[label][id] = sentences\n",
    "    id += 1\n",
    "bp(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  03530.com: { 7515: 371, 7516: 242, 7517: 305, 7518: 451, 7519: 380, ..., 7625: 678, 7626: 496, 7627: 997, 7628: 604, 7629: 604 },\n",
      "  1st-guide.org: { 7233: 684, 7234: 770, 7235: 675, 7236: 495, 7237: 675, ..., 7344: 800, 7345: 733, 7346: 545, 7347: 1311, 7348: 636 },\n",
      "  adrienneâ€¢livejournal.com: { 2013: 208, 2014: 806 },\n",
      "  amandaâ€¢livejournal.com: { 10871: 414, 10872: 121, 10873: 144, 10874: 277, 10875: 582, ..., 10958: 426, 10959: 285, 10960: 221, 10961: 2170, 10962: 223 },\n",
      "  amandaâ€¢wordpress.com: { 3499: 211, 3500: 143, 3501: 521, 3502: 401, 3503: 1440, ..., 3514: 620, 3515: 307, 3516: 377, 3517: 154, 3518: 96 },\n",
      "  ...,\n",
      "  weblogsinc.com: { 5134: 475, 5135: 368, 5136: 252, 5137: 270, 5138: 817, ..., 5182: 579, 5183: 134, 5184: 675, 5185: 238, 5186: 551 },\n",
      "  weightless dollsâ€¢livejournal.com: { 7349: 287, 7350: 265, 7351: 125, 7352: 227, 7353: 103, ..., 7363: 656, 7364: 197, 7365: 607, 7366: 251, 7367: 179 },\n",
      "  wired.com: { 13224: 1619, 13225: 289, 13226: 346, 13227: 337, 13228: 958, ..., 13403: 579, 13404: 204, 13405: 471, 13406: 13883, 13407: 244 },\n",
      "  xanga.com: { 10689: 449, 10690: 442, 10691: 209, 10692: 979, 10693: 463, ..., 10866: 631, 10867: 321, 10868: 630, 10869: 1257, 10870: 504 },\n",
      "  yachtchartersmagazine.com: { 3049: 761, 3050: 1102, 3051: 750, 3052: 854, 3053: 858, ..., 3472: 1000, 3473: 1110, 3474: 1137, 3475: 1039, 3476: 774 }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create a TokenCount-structure mapping class -> id -> tokens count:\n",
    "idTokensCount = dict()\n",
    "for label, docs in documents.items():\n",
    "    if label not in idTokensCount:\n",
    "        idTokensCount[label] = dict()\n",
    "    for id in docs:\n",
    "        idTokensCount[label][id] = len(flattenLists(docs[id]))\n",
    "bp(idTokensCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  livejournal.com d=1500, s=53323, t=894781,\n",
      "  ning.com d=496, s=11458, t=249702,\n",
      "  blog.com d=491, s=14389, t=296090,\n",
      "  salon.com d=480, s=19933, t=443980,\n",
      "  baltimoresun.com d=475, s=14447, t=321028,\n",
      "  blogs.com d=454, s=11700, t=240211,\n",
      "  yachtchartersmagazine.com d=428, s=15283, t=474785,\n",
      "  blogdig.net d=424, s=6146, t=140883,\n",
      "  intl.in d=406, s=12449, t=265879,\n",
      "  mercurynews.com d=402, s=11328, t=268828,\n",
      "  ...,\n",
      "  egl community salesâ€¢livejourna d=12, s=196, t=4138,\n",
      "  cynthiaâ€¢livejournal.com d=12, s=266, t=5191,\n",
      "  seductivegirlâ€¢livejournal.com d=11, s=307, t=6240,\n",
      "  joyâ€¢livejournal.com d=11, s=278, t=4597,\n",
      "  dreamsandpeopleâ€¢livejournal.co d=11, s=243, t=6250,\n",
      "  runaway talesâ€¢livejournal.com d=10, s=700, t=9924,\n",
      "  listannabelâ€¢livejournal.com d=10, s=383, t=6425,\n",
      "  cinieâ€¢wordpress.com d=9, s=254, t=5591,\n",
      "  adrienneâ€¢livejournal.com d=2, s=69, t=1014,\n",
      "  nikiâ€¢livejournal.com d=2, s=55, t=736\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Display statistics (docs counct, sentences count, tokens count) for each class:\n",
    "stats = []\n",
    "for label, docs in documents.items():\n",
    "    current = \"\"\n",
    "    current += label[:30] + \" \"\n",
    "    current += \"d=\" + str(len(docs)) + \", \"\n",
    "    sentenceCount = 0\n",
    "    tokensCount = 0\n",
    "    for id, doc in docs.items():\n",
    "        for sentence in doc:\n",
    "            sentenceCount += 1\n",
    "            for token in sentence:\n",
    "                tokensCount += 1\n",
    "    current += \"s=\" + str(sentenceCount) + \", t=\" + str(tokensCount)\n",
    "    stats.append(current)\n",
    "bp(sorted(stats, key=lambda x: getAllNumbers(x)[-3], reverse=True), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate buckets of documents:\n",
    "\n",
    "You can run the generation of multiple set of buckets in parallel with different parameter (`maxLabelsPerBucket`, `tokensPerBucket`, `maxVarianceRatio`, `maxConsecutiveBadBucketCount` and `maxConsecutiveNoChangeCount`) and keep the set having a few ids deletion and a few number of buckets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Remaining labels: 104\n",
      "Remaining tokens: 9525801 (100.0%)\n",
      "Allocated labels: 0\n",
      "This bucket is not a valid bucket...\n",
      "\n",
      "\n",
      "This bucket is not ADDED\n",
      "\n",
      "\n",
      "--------------------\n",
      "Remaining labels: 104\n",
      "Remaining tokens: 9525801 (100.0%)\n",
      "Allocated labels: 0\n",
      "This bucket is not a valid bucket...\n",
      "\n",
      "\n",
      "This bucket is not ADDED\n",
      "\n",
      "\n",
      "--------------------\n",
      "Remaining labels: 104\n",
      "Remaining tokens: 9525801 (100.0%)\n",
      "Allocated labels: 0\n",
      "This bucket is not a valid bucket...\n",
      "\n",
      "\n",
      "This bucket is not ADDED\n",
      "\n",
      "\n",
      "We removed 2013 (208 tokens) from adrienneâ€¢livejournal.com in remaining\n",
      "We removed 2014 (806 tokens) from adrienneâ€¢livejournal.com in remaining\n",
      "--------------------\n",
      "Remaining labels: 103\n",
      "Remaining tokens: 9524787 (99.98%)\n",
      "Allocated labels: 0\n",
      "This bucket is not a valid bucket...\n",
      "\n",
      "\n",
      "This bucket is not ADDED\n",
      "\n",
      "\n",
      "We removed 12019 (326 tokens) from nikiâ€¢livejournal.com in remaining\n",
      "We removed 12020 (410 tokens) from nikiâ€¢livejournal.com in remaining\n",
      "--------------------\n",
      "Remaining labels: 102\n",
      "Remaining tokens: 9524051 (99.98%)\n",
      "Allocated labels: 0\n",
      "--------------------\n",
      "Remaining labels: 102\n",
      "Remaining tokens: 9417003 (98.85%)\n",
      "Allocated labels: 26\n",
      "--------------------\n",
      "Remaining labels: 83\n",
      "Remaining tokens: 9353741 (98.19%)\n",
      "Allocated labels: 26\n",
      "--------------------\n",
      "Remaining labels: 76\n",
      "Remaining tokens: 9180873 (96.37%)\n",
      "Allocated labels: 45\n",
      "--------------------\n",
      "Remaining labels: 66\n",
      "Remaining tokens: 8873778 (93.15%)\n",
      "Allocated labels: 52\n",
      "--------------------\n",
      "Remaining labels: 56\n",
      "Remaining tokens: 8436271 (88.56%)\n",
      "Allocated labels: 62\n",
      "--------------------\n",
      "Remaining labels: 45\n",
      "Remaining tokens: 7749969 (81.35%)\n",
      "Allocated labels: 72\n",
      "--------------------\n",
      "Remaining labels: 38\n",
      "Remaining tokens: 6922068 (72.66%)\n",
      "Allocated labels: 83\n",
      "--------------------\n",
      "Remaining labels: 30\n",
      "Remaining tokens: 6110694 (64.14%)\n",
      "Allocated labels: 90\n",
      "--------------------\n",
      "Remaining labels: 29\n",
      "Remaining tokens: 5156184 (54.12%)\n",
      "Allocated labels: 98\n",
      "--------------------\n",
      "Remaining labels: 22\n",
      "Remaining tokens: 4270531 (44.83%)\n",
      "Allocated labels: 99\n",
      "--------------------\n",
      "Remaining labels: 19\n",
      "Remaining tokens: 3516163 (36.91%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 17\n",
      "Remaining tokens: 2805848 (29.45%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 13\n",
      "Remaining tokens: 2245131 (23.56%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 12\n",
      "Remaining tokens: 1817371 (19.07%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 7\n",
      "Remaining tokens: 1478849 (15.52%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 5\n",
      "Remaining tokens: 1295393 (13.59%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 5\n",
      "Remaining tokens: 1141913 (11.98%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 4\n",
      "Remaining tokens: 1012862 (10.63%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 4\n",
      "Remaining tokens: 897622 (9.42%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 4\n",
      "Remaining tokens: 782470 (8.21%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 4\n",
      "Remaining tokens: 667593 (7.0%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 3\n",
      "Remaining tokens: 569367 (5.97%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 2\n",
      "Remaining tokens: 515515 (5.41%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 2\n",
      "Remaining tokens: 477056 (5.0%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 2\n",
      "Remaining tokens: 438611 (4.6%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 2\n",
      "Remaining tokens: 400188 (4.2%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 2\n",
      "Remaining tokens: 361730 (3.79%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 2\n",
      "Remaining tokens: 323323 (3.39%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 2\n",
      "Remaining tokens: 284939 (2.99%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 2\n",
      "Remaining tokens: 246497 (2.58%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 2\n",
      "Remaining tokens: 208042 (2.18%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 2\n",
      "Remaining tokens: 169681 (1.78%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 2\n",
      "Remaining tokens: 131268 (1.37%)\n",
      "Allocated labels: 102\n",
      "--------------------\n",
      "Remaining labels: 1\n",
      "Remaining tokens: 120414 (1.26%)\n",
      "Allocated labels: 102\n",
      "\n",
      "\n",
      "This bucket is not ADDED\n",
      "\n",
      "\n",
      "--------------------\n",
      "Remaining labels: 1\n",
      "Remaining tokens: 120414 (1.26%)\n",
      "Allocated labels: 102\n",
      "\n",
      "\n",
      "This bucket is not ADDED\n",
      "\n",
      "\n",
      "--------------------\n",
      "Remaining labels: 1\n",
      "Remaining tokens: 120414 (1.26%)\n",
      "Allocated labels: 102\n",
      "\n",
      "\n",
      "This bucket is not ADDED\n",
      "\n",
      "\n",
      "We removed 1209 (120414 tokens) from scienceblogs.com in remaining\n",
      "eliminatedLabels:\n",
      "{ }\n",
      "eliminatedIds:\n",
      "{\n",
      "  'adrienneâ€¢livejournal.com': { '2013': 208, '2014': 806 },\n",
      "  'nikiâ€¢livejournal.com': { '12019': 326, '12020': 410 },\n",
      "  'scienceblogs.com': { '1209': 120414 }\n",
      "}\n",
      "Count of labels in eliminatedIds: 3\n",
      "Count of ids in eliminatedIds: 5\n"
     ]
    }
   ],
   "source": [
    "# Generate buckets:\n",
    "buckets = makeBuckets(idTokensCount, tokensPerBucket=1e6, maxLabelsPerBucket=len(documents) / 4, maxVarianceRatio=0.2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    baltimoresun.com: { 12340: 437, 12356: 865, 12361: 1142, 12363: 1370, 12370: 2330, ..., 12731: 1030, 12756: 934, 12760: 404, 12776: 368, 12792: 1038 },\n",
      "    blog.com: { 1525: 216, 1528: 303, 1532: 437, 1533: 326, 1535: 1061, ..., 1979: 489, 1981: 135, 1991: 255, 2008: 269, 2012: 757 },\n",
      "    blogdig.net: { 2392: 104, 2393: 384, 2395: 118, 2418: 124, 2419: 102, ..., 2810: 656, 2811: 111, 2812: 333, 2813: 116, 2814: 475 },\n",
      "    blogs.com: { 3526: 353, 3532: 337, 3534: 107, 3552: 110, 3565: 1707, ..., 3942: 101, 3949: 124, 3950: 2271, 3966: 161, 3967: 319 },\n",
      "    google.com: { 4625: 149, 4627: 149, 4630: 895, 4642: 352, 4646: 174, ..., 4855: 178, 4856: 154, 4860: 885, 4864: 297, 4870: 703 },\n",
      "    ...,\n",
      "    techcrunch.com: { 14411: 524, 14413: 440, 14415: 611, 14416: 377, 14420: 407, ..., 14568: 414, 14574: 641, 14576: 102, 14582: 2246, 14584: 488 },\n",
      "    technophobiac.net: { 11638: 419, 11656: 654, 11659: 801, 11665: 379, 11674: 503, ..., 12001: 749, 12006: 906, 12007: 531, 12016: 389, 12017: 650 },\n",
      "    thinkprogress.org: { 7742: 432, 7743: 424, 7746: 785, 7749: 294, 7752: 500, ..., 7851: 2068, 7855: 1215, 7864: 958, 7880: 1134, 7887: 893 },\n",
      "    wired.com: { 13227: 337, 13228: 958, 13230: 494, 13231: 566, 13238: 323, ..., 13379: 100, 13385: 509, 13387: 1364, 13396: 2366, 13406: 13883 },\n",
      "    yachtchartersmagazine.com: { 3057: 1703, 3072: 1000, 3085: 685, 3093: 1500, 3095: 1028, ..., 3367: 1014, 3370: 1157, 3396: 1095, 3428: 903, 3473: 1110 }\n",
      "  },\n",
      "  {\n",
      "    baltimoresun.com: { 12348: 276, 12389: 1258, 12392: 764, 12402: 702, 12421: 2941, ..., 12780: 261, 12799: 393, 12804: 265, 12805: 165, 12807: 503 },\n",
      "    blog.com: { 1522: 664, 1540: 210, 1544: 785, 1554: 224, 1556: 142, ..., 1987: 780, 1997: 449, 2002: 686, 2007: 363, 2010: 216 },\n",
      "    blogdig.net: { 2394: 860, 2398: 209, 2406: 106, 2408: 1933, 2411: 111, ..., 2802: 87, 2803: 1070, 2805: 620, 2811: 111, 2814: 475 },\n",
      "    blogs.com: { 3545: 569, 3551: 246, 3553: 340, 3559: 177, 3568: 997, ..., 3920: 453, 3949: 124, 3953: 179, 3957: 446, 3970: 237 },\n",
      "    google.com: { 4621: 224, 4632: 218, 4635: 239, 4645: 253, 4646: 174, ..., 4839: 378, 4857: 2831, 4869: 337, 4873: 439, 4876: 327 },\n",
      "    ...,\n",
      "    techcrunch.com: { 14407: 170, 14410: 462, 14418: 964, 14419: 444, 14420: 407, ..., 14575: 435, 14576: 102, 14578: 338, 14584: 488, 14586: 339 },\n",
      "    technophobiac.net: { 11632: 533, 11639: 624, 11643: 604, 11658: 730, 11664: 644, ..., 11999: 1432, 12000: 713, 12009: 496, 12015: 429, 12016: 389 },\n",
      "    thinkprogress.org: { 7744: 580, 7745: 436, 7753: 1301, 7756: 1201, 7758: 2092, ..., 7859: 2439, 7871: 1389, 7880: 1134, 7882: 1884, 7885: 1822 },\n",
      "    wired.com: { 13226: 346, 13227: 337, 13228: 958, 13232: 752, 13234: 724, ..., 13392: 1056, 13394: 1543, 13396: 2366, 13398: 808, 13404: 204 },\n",
      "    yachtchartersmagazine.com: { 3074: 1326, 3097: 867, 3134: 742, 3185: 839, 3218: 1579, ..., 3435: 747, 3436: 664, 3456: 929, 3458: 865, 3465: 962 }\n",
      "  }\n",
      "]\n",
      "Number of buckets: 34\n",
      "Labels:\n",
      "\tmin: intl.in\n",
      "\tmax: blogs.com\n",
      "Tokens count:\n",
      "\tmin: 38179\n",
      "\tmax: 38460\n",
      "\tmean: 38393\n",
      "\ttotal: 0.99M\n",
      "Ids count:\n",
      "\tmin: 56\n",
      "\tmax: 52\n",
      "\tmean: 56\n",
      "\ttotal: 1456\n",
      "Is it a valid bucket?\n",
      "\tYES\n",
      "\n",
      "\n",
      "\n",
      "Labels:\n",
      "\tmin: intl.in\n",
      "\tmax: thinkprogress.org\n",
      "Tokens count:\n",
      "\tmin: 38197\n",
      "\tmax: 38460\n",
      "\tmean: 38402\n",
      "\ttotal: 0.99M\n",
      "Ids count:\n",
      "\tmin: 57\n",
      "\tmax: 29\n",
      "\tmean: 57\n",
      "\ttotal: 1494\n",
      "Is it a valid bucket?\n",
      "\tYES\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print some stats on generated buckets:\n",
    "bp(buckets[-2:])\n",
    "print(\"Number of buckets: \" + str(len(buckets)))\n",
    "for bucket in buckets[-2:]:\n",
    "    bucketStats(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of black n-grams from each bucket and merge\n",
    "\n",
    "This can be done in parallel if your corpus is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2% [                    ]\n",
      "  8% [=                   ] (10m 49.863s left)\n",
      " 17% [===                 ] (10m 45.026s left)\n",
      " 26% [=====               ] (11m 8.722s left)\n",
      " 35% [=======             ] (10m 28.338s left)\n",
      " 44% [========            ] (8m 57.269s left)\n",
      " 52% [==========          ] (7m 46.026s left)\n",
      " 61% [============        ] (6m 27.876s left)\n",
      " 70% [==============      ] (4m 51.141s left)\n",
      " 79% [===============     ] (3m 27.539s left)\n",
      " 88% [=================   ] (2m 0.41s left)\n",
      " 97% [=================== ] (30.469s left)\n",
      "100% [====================] (total duration: 17m 19.94s, mean duration: 30.586s)\n",
      "{\n",
      "  03530.com: { ' nasal spray, ' zomig, ..., zomig ', zomig ' nasal },\n",
      "  1st-guide.org: { , d -, , g, ..., zinc, zinc countertops },\n",
      "  amandaâ€¢livejournal.com: { \" aoi, \" uruha, ..., you wish you, your animation },\n",
      "  amandaâ€¢wordpress.com: { , i no, , o my, ..., wounds and, wounds and issues },\n",
      "  angelliluâ€¢livejournal.com: { - yos ;, a jerk, ..., yo -, yo - yos },\n",
      "  ...,\n",
      "  weblogsinc.com: { ( nasdaq, ( nasdaq :, ..., wal - mart, wall street },\n",
      "  weightless dollsâ€¢livejournal.com: { \" what is, , from my, ..., your problem, your problem ? },\n",
      "  wired.com: { ! eckhardt this, ! put, ..., zucker, zuckerberg },\n",
      "  xanga.com: { \" fleece, \" fleece \", ..., your therapist will, zolpidem },\n",
      "  yachtchartersmagazine.com: { ! ( tm, ! nosql, ..., zacks equity research, zealand pharma }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "bngrams = dict()\n",
    "for bucket in pb(shuffle(buckets)):\n",
    "    # First we make class-documents:\n",
    "    cds, labels = [], []\n",
    "    for label, ids in bucket.items():\n",
    "        ids = list(ids.keys())\n",
    "        labels.append(label)\n",
    "        cd = []\n",
    "        for id in ids:\n",
    "            cd += documents[label][id]\n",
    "        cds.append(cd)\n",
    "    # Then we compute TFIDF with a ngrams range 1 to 3:\n",
    "    tfidf = TFIDF(cds, doLower=True, ngramRange=(1, 3), minDF=1, verbose=False)\n",
    "    # Then we generate bngrams:\n",
    "    bns = tfidf.getBlackNgrams(0.3)\n",
    "    # And we merge it in the main black n-grams structure (bngrams):\n",
    "    for i in range(len(labels)):\n",
    "        currentLabel = labels[i]\n",
    "        currentBNS = bns[i]\n",
    "        if currentLabel not in bngrams:\n",
    "            bngrams[currentLabel] = set()\n",
    "        for bn in currentBNS:\n",
    "            bngrams[currentLabel].add(bn)\n",
    "bp(bngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deletion of sentences\n",
    "\n",
    "Can be done in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0% [                    ]\n",
      "  9% [=                   ] (1m 54.817s left)\n",
      " 19% [===                 ] (1m 31.5s left)\n",
      " 29% [=====               ] (1m 24.063s left)\n",
      " 39% [=======             ] (1m 17.153s left)\n",
      " 49% [=========           ] (1m 2.986s left)\n",
      " 59% [===========         ] (52.369s left)\n",
      " 69% [=============       ] (41.194s left)\n",
      " 79% [===============     ] (27.234s left)\n",
      " 89% [=================   ] (13.751s left)\n",
      " 99% [=================== ] (0.071s left)\n",
      "100% [====================] (total duration: 2m 14.699s, mean duration: 0.008s)\n"
     ]
    }
   ],
   "source": [
    "# Deletion:\n",
    "filteredDataset = []\n",
    "for label, sentences in pb(dataset):\n",
    "    if label in bngrams:\n",
    "        newSentences = []\n",
    "        bns = bngrams[label]\n",
    "        for sentence in sentences:\n",
    "            ngrams = extractNgrams([sentence], ngrams=1, doLower=True)\n",
    "            ngrams = ngrams.union(extractNgrams([sentence], ngrams=2, doLower=True))\n",
    "            ngrams = ngrams.union(extractNgrams([sentence], ngrams=3, doLower=True))\n",
    "            foundBlack = False\n",
    "            for bn in bns:\n",
    "                if bn in ngrams:\n",
    "                    foundBlack = True\n",
    "                    break\n",
    "            if not foundBlack:\n",
    "                newSentences.append(sentence)\n",
    "        filteredDataset.append((label, newSentences))\n",
    "    else:\n",
    "        filteredDataset.append((label, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original doc:\n",
      "Uttar Pradesh Health Minister Anant Kumar Misra Tuesday blamed the central government for the various diseases threatening to take on an epidemic proportion in the state.\n",
      "He sought to blame the central government not only for the \"inadequate supply\" but also \"substandard\" and \"delayed\" supply of the required vaccines for preventing diseases like Japanese Encephalitis, viral encephalitis, malaria as well other vector borne diseases.\n",
      "The minister, a cousin of the all powerful Bahujan Samaj Party (BSP) national general secretary Satish Chandra Misra, also accused the government of \"failure to maintain the cold chain for storage of various vaccines in the state\"\n",
      "He admitted that there were as many as __int_3__ deaths on account of the spread of \"acute encephalitis in Gorakhpur, Basti, Maharajganj, Siddharthnagar, Sant Kabir Nagar, Kushinagar, Bahraich spread across eastern Uttar Pradesh\"\n",
      "According to him, we have detected about cases of acute encephalitis in these districts so far.\n",
      "Addressing a press conference here Tuesday, Misra said, not only has UP been deprived of the required quantity of vaccines needed by the state to tackle the diseases, but it had failed to make arrangements for the necessary cold chain desired for storage of all vaccines.\n",
      "According to Misra, ignoring our additional demand for one million JE vaccines, the centre provided us with just about, leaving a huge gap that was responsible for spread of the diseases in about seven districts of eastern UP.\n",
      "Later while admitting that the state had \"received about __int_2__ million vaccines\" he claimed that \"as many as of these were substandard and could not be used\"\n",
      "\n",
      "Filtered doc:\n",
      "According to him, we have detected about cases of acute encephalitis in these districts so far.\n",
      "Addressing a press conference here Tuesday, Misra said, not only has UP been deprived of the required quantity of vaccines needed by the state to tackle the diseases, but it had failed to make arrangements for the necessary cold chain desired for storage of all vaccines.\n",
      "According to Misra, ignoring our additional demand for one million JE vaccines, the centre provided us with just about, leaving a huge gap that was responsible for spread of the diseases in about seven districts of eastern UP.\n",
      "Later while admitting that the state had \"received about __int_2__ million vaccines\" he claimed that \"as many as of these were substandard and could not be used\"\n"
     ]
    }
   ],
   "source": [
    "# We display 2 docs (not filtered and filtered):\n",
    "for i in range(len(dataset)):\n",
    "    doc1 = dataset[i][1]\n",
    "    doc2 = filteredDataset[i][1]\n",
    "    if len(doc2) < len(doc1) - 2 and len(doc2) > 3 and len(doc2) < 5:\n",
    "        print(\"Original doc:\")\n",
    "        print(detokenize(doc1))\n",
    "        print(\"\\nFiltered doc:\")\n",
    "        print(detokenize(doc2))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  03530.com: 1,\n",
      "  baltimoresun.com: 2,\n",
      "  blog.com: 2,\n",
      "  blogdig.net: 2,\n",
      "  climateark.org: 8,\n",
      "  downloadsquad.com: 1,\n",
      "  dragonspam_ post your dragcave eggsâ€¢livejournal.com: 4,\n",
      "  ...,\n",
      "  techcrunch.com: 2,\n",
      "  technophobiac.net: 4,\n",
      "  theoffside.com: 1,\n",
      "  trendaz.com: 11,\n",
      "  web-directory.in: 1,\n",
      "  xanga.com: 2,\n",
      "  yachtchartersmagazine.com: 19\n",
      "}\n",
      "209 docs deleted.\n"
     ]
    }
   ],
   "source": [
    "# Remove empty docs:\n",
    "newFilteredDataset = []\n",
    "labelsOfEmptyDocs = dict()\n",
    "count = 0\n",
    "for label, doc in filteredDataset:\n",
    "    if doc is None or len(doc) == 0:\n",
    "        if label not in labelsOfEmptyDocs:\n",
    "            labelsOfEmptyDocs[label] = 0\n",
    "        labelsOfEmptyDocs[label] += 1\n",
    "        count += 1\n",
    "    else:\n",
    "        newFilteredDataset.append((label, doc))\n",
    "bp(labelsOfEmptyDocs, 3)\n",
    "print(str(count) + \" docs deleted.\")\n",
    "filteredDataset = newFilteredDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  (\n",
      "    cinieâ€¢wordpress.com,\n",
      "    [ [ The, renegade, ..., statement, . ], [ The, nerve, ..., people, ! ], ..., [ Founded, in, ..., nation, . ], [ Which, corrupt, ..., ask, ? ] ]\n",
      "  ),\n",
      "  (\n",
      "    cinieâ€¢wordpress.com,\n",
      "    [ [ Face, it, ..., toast, . ], [ Running, as, ..., fail, . ], ..., [ No, gimmick, ..., ticket, . ], [ That, clearly, ..., math, . ] ]\n",
      "  ),\n",
      "  ...,\n",
      "  (\n",
      "    trendaz.com,\n",
      "    [ [ \", Three, ..., added, . ], [ However, ,, ..., as, __int_2__ ], ..., [ Security, forces, ..., anonymity, . ], [ The, country, ..., success, . ] ]\n",
      "  ),\n",
      "  (\n",
      "    trendaz.com,\n",
      "    [ [ \", He, ..., Post, . ], [ \", It, ..., disgusting, . ], [ Mike, Webster, ..., issues, . ], [ Police, filmed, ..., authorities, . ] ]\n",
      "  )\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Print of the final result:\n",
    "bp(filteredDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentencesCount: 447792\n",
      "fSentencesCount: 296522\n",
      "33.7% deleted\n"
     ]
    }
   ],
   "source": [
    "# Display of % deleted sentence (a little bit more than the deletion ratio of 0.3):\n",
    "sentencesCount = 0\n",
    "fSentencesCount = 0\n",
    "for label, doc in dataset:\n",
    "    sentencesCount += len(doc)\n",
    "for label, doc in filteredDataset:\n",
    "    fSentencesCount += len(doc)\n",
    "print(\"sentencesCount: \" + str(sentencesCount))\n",
    "print(\"fSentencesCount: \" + str(fSentencesCount))\n",
    "print(str((sentencesCount - fSentencesCount) / sentencesCount * 100)[:4] + \"% deleted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
